{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive summarization\n",
    "\n",
    "Abstractive summarization is where the algorithm doesn't just pick sentences from the original, but it synthesizes a summary from the text, potentially also including words or phrases that were not present in the original text.\n",
    "\n",
    "There are multiple github projects where people have implemented abstract summarizers using RNNs, so I'll be trying to use them here.\n",
    "\n",
    "Github abstract summarizers:\n",
    "* [Pytorch implementation](https://github.com/Iwontbecreative/Abstractive-summarization-OpenNMT)\n",
    "* [Another pytorch implementations, with better documentation](https://github.com/alesee/abstractive-text-summarization)\n",
    "* [Tensorflow implementation (summarizes Amazon food reviews)](https://github.com/JRC1995/Abstractive-Summarization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with trying to implement the first Pytorch implementation by 'Iwontbecreative', on top of OpenNMT.\n",
    "\n",
    "The first thing that it wants is it wants four text files: it wants source and target training text files, and then it also wants source and target validation files. In this case, the source will be the full CNN story, and the target will be the highlights of the story. I therefore need to process 100 of these CNN stories, and then I can split them into train/validation of 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_100_stories.txt\n",
      "CONTRIBUTORS.md\n",
      "Dockerfile\n",
      "LICENSE.md\n",
      "README.md\n",
      "data\n",
      "docs\n",
      "mkdocs.yml\n",
      "onmt\n",
      "opts.py\n",
      "preprocess.py\n",
      "requirements.txt\n",
      "setup.py\n",
      "test\n",
      "tools\n",
      "train.py\n",
      "train_nb.ipynb\n",
      "translate.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls ../data/Abstractive-summarization-OpenNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll open the CNN_100_stories.txt file, put it into an array, and then use that to process the CNN stories\n",
    "## into source and target.\n",
    "\n",
    "with open(\"../data/Abstractive-summarization-OpenNMT/CNN_100_stories.txt\", \"r\") as f:\n",
    "    CNN_100_stories = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_100_stories_filenames = CNN_100_stories.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1509.00685.pdf\n",
      "1609.07034.pdf\n",
      "1706.06681.pdf\n",
      "1708.00625.pdf\n",
      "1711.09357.pdf\n",
      "1807.00199.pdf\n",
      "2744372.pdf\n",
      "80_2_6_0.pdf\n",
      "Feature-Based-Time-Series-Analysis_Rob-Hyndman.pdf\n",
      "GoogleNews-vectors-negative300.bin\n",
      "Hands-On-Natural-Language-Processing-with-Python\n",
      "Simons-Institute-Manning-2017.pdf\n",
      "W17-2307.pdf\n",
      "cnn\n",
      "eaao5580.full.pdf\n",
      "ed3book.pdf\n",
      "glove.6B\n",
      "glove.6B.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls ../../../resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CNN_STORIES = \"../../../resources/cnn/stories/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = \"\"\n",
    "tgt_train = \"\"\n",
    "src_val = \"\"\n",
    "tgt_val = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lines = []\n",
    "tgt_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CNN_100_stories_filenames)):\n",
    "    with open(PATH_TO_CNN_STORIES+CNN_100_stories_filenames[i], \"rb\") as f:\n",
    "        text = f.read().decode(\"utf-8\")\n",
    "        temp = text.split(\"@highlight\")\n",
    "        article = re.sub(r'\\s+', ' ', temp[0].strip())\n",
    "        src_lines.append(article)\n",
    "\n",
    "        highlights = [re.sub(r'\\n', '', highlight) for highlight in temp[1:]]\n",
    "        highlights = \". \".join(highlights)\n",
    "        tgt_lines.append(highlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the src lines and target lines into a train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_lines = src_lines[0:80]\n",
    "tgt_train_lines = tgt_lines[0:80]\n",
    "\n",
    "src_val_lines = src_lines[80:]\n",
    "tgt_val_lines = tgt_lines[80:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's join the lines together for each list, and then write these to text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_CNN = \"\\n\".join(src_train_lines)\n",
    "tgt_train_CNN = \"\\n\".join(tgt_train_lines)\n",
    "\n",
    "src_val_CNN = \"\\n\".join(src_val_lines)\n",
    "tgt_val_CNN = \"\\n\".join(tgt_val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "morph\n",
      "src-test.txt\n",
      "src-train.txt\n",
      "src-val.txt\n",
      "test_model2.src\n",
      "test_model2.tgt\n",
      "tgt-train.txt\n",
      "tgt-val.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls ../data/Abstractive-summarization-OpenNMT/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Abstractive-summarization-OpenNMT/data/src-train-CNN.txt\", \"w\") as f:\n",
    "    f.write(src_train_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Abstractive-summarization-OpenNMT/data/tgt-train-CNN.txt\", \"w\") as f:\n",
    "    f.write(tgt_train_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Abstractive-summarization-OpenNMT/data/src-val-CNN.txt\", \"w\") as f:\n",
    "    f.write(src_val_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Abstractive-summarization-OpenNMT/data/tgt-val-CNN.txt\", \"w\") as f:\n",
    "    f.write(tgt_val_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so overall I think that I have probably 3 viable options:\n",
    "\n",
    "1. Instead of the OpenNMT implementation that Iwontbecreative has in their github project, I think that I had better clone OpenNMT directly and follow their [example](http://opennmt.net/OpenNMT-py/Summarization.html) here on how to do abstractive summarization with the most up to date version of OpenNMT.\n",
    "2. I can try the model implemented by 'Hand on NLP with Python' which uses a Tensorflow model, and see how that works. I think that I would still want to see if I could use the biomedical word2vec vectors that I downloaded from bio.nlplab.org, and see if that made any difference.\n",
    "    * I could probably try to see whether or not I can even just feed in an article's text and even get a summary out, if that's even something that I can get working without spending copious amounts of time on it.\n",
    "    * Once I can figure out how to even get it to take an article, then I can see whether or not I can figure out how to substitute word2vec vectors for GloVe vectors.\n",
    "3. The third model that I think I can try would be alesee's project [here](https://github.com/alesee/abstractive-text-summarization) on github, and the reason why I think this one might work is honestly mostly because he updated it within the last month, and he also actually has jupyter notebooks in his repository - and presumably these notebooks actually run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
