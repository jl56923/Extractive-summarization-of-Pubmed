{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import pprint\n",
    "\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../API_ignore.txt\", \"r\") as f:\n",
    "    lines = f.read()\n",
    "\n",
    "entrez_api_key = lines.split(\":\")[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "First, we use esearch to send a query for all reviews & systematic reviews that have free full text for a specific topic; we want to get the PMIDs of these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "esearch_base_query = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?\"\n",
    "\n",
    "review_pmids_query_dict = {\n",
    "    \"db\": \"pubmed\",\n",
    "    \"sort\": \"relevance\",\n",
    "    \"retmax\": '100',\n",
    "    \"term\": '{}+AND+(Review[ptyp]+OR+systematic[sb])+AND+free+full+text[sb]+AND+\"last%205%20years\"[PDat]',\n",
    "    \"api_key\": entrez_api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_terms = [k+\"=\"+v for k, v in review_pmids_query_dict.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review_pmids_query = esearch_base_query + \"&\".join(joined_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_query = \"atrial+fibrillation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_for_filename = entity_query.replace(\"+\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&sort=relevance&retmax=100&term=atrial+fibrillation+AND+(Review[ptyp]+OR+systematic[sb])+AND+free+full+text[sb]+AND+\"last%205%20years\"[PDat]&api_key=b0b12c603fda132e7f526bd128008cf75a08'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_review_pmids_query.format(entity_query, entrez_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we send this query to pubmed using eutils, we get back an xml object which we can store in a tree.\n",
    "r = requests.get(get_review_pmids_query.format(entity_query, entrez_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.ElementTree(ET.fromstring(r.content))\n",
    "root = tree.getroot()\n",
    "\n",
    "pmids = root.findall('.//Id')\n",
    "\n",
    "pmid_list = [pmid.text for pmid in pmids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Now that we have the PMIDs for the 20 review papers returned by esearch, we have to convert the PMIDs into PMCIDs. In order to convert the PMIDs to PMCIDs, we have to use the ID converter provided by the NCBI, as outlined here: https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert each PMID into a PMCID. The JSON that is returned from this request always has a key 'records'.\n",
    "# Check the dictionary inside of 'records'; if there is a key called 'errmsg', then you know that\n",
    "# the convert request failed. Otherwise, check to see if the dictionary inside of records has a key called \n",
    "# 'pmcid'. If it does, grab the value of the key 'pmcid' and store it. We'll use that PMCID to query PMC to\n",
    "# fetch the xml of the full paper.\n",
    "\n",
    "convert_PMID_query = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=review_assistant&email=jl56923@gmail.com&ids={}&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23145eb293194cbd8ab5227b1c806a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pmcid_list = []\n",
    "\n",
    "for pmid in log_progress(pmid_list, every=1):\n",
    "    r = requests.get(convert_PMID_query.format(pmid))\n",
    "    result = r.json()\n",
    "    records_dict = result['records'][0]\n",
    "    # If there is an error message in the records dictionary that gets returned with the result, then this\n",
    "    # paper does not have a PMCID and we are not going to be able to get the full text of this paper.\n",
    "    if 'errmsg' in records_dict:\n",
    "        pass\n",
    "    else:\n",
    "        if 'pmcid' in records_dict:\n",
    "            pmcid_list.append(records_dict['pmcid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Now that we have the list of pmcids, we can use efetch to get the xml of these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pmc_xml_query = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id={}&tool=review_assistant&email=jl56923@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so first finding the abstract and then the body nodes, and then going through each of those and joining together the paragraphs seems to work relatively well. We'll go ahead and write the abstract and body texts to files instead. We'll also define a function that can take an XML node, look for all the paragraphs, join them together and return a clean string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_as_clean_string(xml_node):\n",
    "    # first, we'll clear all the children elements of the table-wrap tags,\n",
    "    # which will get rid of all the content that was in the tables.\n",
    "    # But, first check to see if the table even has tables, because if there is no table-wrap tag\n",
    "    # then you don't need to do anything.\n",
    "    if not xml_node:\n",
    "        return \"\"\n",
    "    \n",
    "    if xml_node.find(\".//table-wrap\"):\n",
    "        for table in xml_node.findall(\".//table-wrap\"):\n",
    "            table.clear()\n",
    "    \n",
    "    node_paragraphs = xml_node.findall(\".//p\")\n",
    "\n",
    "    clean_string = \"\"\n",
    "\n",
    "    for paragraph in node_paragraphs:\n",
    "        clean_string += \" \".join(paragraph.itertext())\n",
    "        clean_string += \" \"\n",
    "        \n",
    "    clean_string = clean_string.strip()\n",
    "    \n",
    "    # We'll get rid of anything inside of square brackets, since those tend to be the citations.\n",
    "    clean_string = re.sub(r'\\[.*?]', \"\", clean_string)\n",
    "    clean_string = re.sub(r'(\\s)+', \" \", clean_string)\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text_exclude_after_conclusion(body_node):\n",
    "    sections = body_node.findall(\".//sec\")\n",
    "    \n",
    "    conclusion_index = len(sections)\n",
    "            \n",
    "    for index, section in enumerate(sections):\n",
    "        section_title = section.find(\".//title\")\n",
    "        if section_title:\n",
    "            if section_title.text:\n",
    "                if \"conclusion\" in section_title.text.lower():\n",
    "                    conclusion_index = index\n",
    "                    break\n",
    "                if \"discussion\" in section_title.text.lower():\n",
    "                    conclusion_index = index\n",
    "                    break\n",
    "    \n",
    "    article_text = \"\"\n",
    "    \n",
    "    for section in sections[:conclusion_index]:\n",
    "        article_text += get_paragraphs_as_clean_string(section)\n",
    "    \n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmcid_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(get_pmc_xml_query.format(pmcid_list[4]))\n",
    "\n",
    "# tree = ET.ElementTree(ET.fromstring(r.content))\n",
    "# root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def depth_iter(element, tag=None):\n",
    "#     stack = []\n",
    "#     stack.append(iter([element]))\n",
    "#     while stack:\n",
    "#         e = next(stack[-1], None)\n",
    "#         if e == None:\n",
    "#             stack.pop()\n",
    "#         else:\n",
    "#             stack.append(iter(e))\n",
    "#             if tag == None or e.tag == tag:\n",
    "#                 yield (e, stack.__len__()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perf_func(elem, func, level=0):\n",
    "#     func(elem,level)\n",
    "#     for child in elem.getchildren():\n",
    "#         perf_func(child, func, level+1)\n",
    "\n",
    "# def print_level(elem,level):\n",
    "#     print('-'*level+elem.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_test = root.findall(\".//subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_group_test = root.findall(\".//subj-group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_func(root, print_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2987d43c24d4cbf945bdd419bd97df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=64)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully appended dictionary for PMC4952027 to entity_dictionary_list.\n",
      "Unable to get text for PMC5942796.\n",
      "Successfully appended dictionary for PMC5598874 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC4766963 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5658096 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5560908 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5089715 to entity_dictionary_list.\n",
      "Unable to get text for PMC5071280.\n",
      "Successfully appended dictionary for PMC5079045 to entity_dictionary_list.\n",
      "Unable to get text for PMC5427484.\n",
      "Successfully appended dictionary for PMC5543536 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5340010 to entity_dictionary_list.\n",
      "Unable to get text for PMC5122472.\n",
      "Successfully appended dictionary for PMC5442605 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5380695 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5286679 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5321114 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5382469 to entity_dictionary_list.\n",
      "Unable to get text for PMC5500874.\n",
      "Successfully appended dictionary for PMC5465041 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC4599513 to entity_dictionary_list.\n",
      "Unable to get text for PMC5346472.\n",
      "Unable to get text for PMC5585859.\n",
      "Unable to get text for PMC5933600.\n",
      "Successfully appended dictionary for PMC4937957 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5752005 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5586302 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5331111 to entity_dictionary_list.\n",
      "Unable to get text for PMC5046840.\n",
      "Successfully appended dictionary for PMC4819630 to entity_dictionary_list.\n",
      "Unable to get text for PMC4547665.\n",
      "Unable to get text for PMC4547682.\n",
      "Successfully appended dictionary for PMC5704695 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5622555 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5345987 to entity_dictionary_list.\n",
      "Unable to get text for PMC5108192.\n",
      "Unable to get text for PMC4777910.\n",
      "Unable to get text for PMC4189345.\n",
      "Unable to get text for PMC4732179.\n",
      "Unable to get text for PMC4547664.\n",
      "Unable to get text for PMC4547683.\n",
      "Unable to get text for PMC4472367.\n",
      "Unable to get text for PMC4110594.\n",
      "Successfully appended dictionary for PMC5656712 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC4742513 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5482349 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC4630199 to entity_dictionary_list.\n",
      "Unable to get text for PMC4764082.\n",
      "Successfully appended dictionary for PMC5588987 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC4642960 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC4788372 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5457732 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5487882 to entity_dictionary_list.\n",
      "Successfully appended dictionary for PMC5382449 to entity_dictionary_list.\n",
      "Unable to get text for PMC4814009.\n",
      "Unable to get text for PMC4246362.\n",
      "Unable to get text for PMC4724415.\n",
      "Unable to get text for PMC5403606.\n",
      "Unable to get text for PMC5322057.\n",
      "Successfully appended dictionary for PMC4957677 to entity_dictionary_list.\n",
      "Unable to get text for PMC4329775.\n",
      "Unable to get text for PMC4731871.\n",
      "Unable to get text for PMC4547684.\n",
      "Successfully appended dictionary for PMC5234257 to entity_dictionary_list.\n"
     ]
    }
   ],
   "source": [
    "entity_dictionary_list = []\n",
    "\n",
    "for i in log_progress(range(len(pmcid_list)), every=1):\n",
    "    \n",
    "    r = requests.get(get_pmc_xml_query.format(pmcid_list[i]))\n",
    "\n",
    "    tree = ET.ElementTree(ET.fromstring(r.content))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Check to see if you can even get the XML from PMC; if not, then pass. If you can, then you can go ahead\n",
    "    # and continue making the dictionary and pickling it.\n",
    "    body = root.find(\".//body\")\n",
    "    if body:\n",
    "        article_text = get_article_text_exclude_after_conclusion(body)\n",
    "    else:\n",
    "        print(f\"Unable to get text for {pmcid_list[i]}.\")\n",
    "        continue\n",
    "    \n",
    "    pmcid = pmcid_list[i]\n",
    "    \n",
    "    # We'll find the first article-title in the text, which should be the title of the paper.\n",
    "    title = root.find(\".//article-title\").text\n",
    "    \n",
    "    keyword_elements = root.findall(\".//kwd\")\n",
    "#     #\n",
    "#     if keyword_elements is None:\n",
    "#         keyword_elements = root.findall(\".//subject\")\n",
    "#         keyword_elements = [keyword_element for keyword_element in keyword_elements if not len(list(keyword_element))]\n",
    "#     #\n",
    "    if keyword_elements:\n",
    "        keywords = [keyword.text.lower() for keyword in keyword_elements if keyword.text]\n",
    "    else:\n",
    "        keywords = []\n",
    "    \n",
    "    abstract = root.find(\".//abstract\")\n",
    "    abstract_text = get_paragraphs_as_clean_string(abstract)\n",
    "    \n",
    "    citations = root.findall(\".//pub-id\")\n",
    "    citation_tuples = [(citation.text, list(citation.attrib.values())[0]) for citation in citations]\n",
    "    \n",
    "    paper_dict = {\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"keywords\": keywords,\n",
    "        \"abstract_text\": abstract_text,\n",
    "        \"article_text\": article_text,\n",
    "        \"citation_tuples\": citation_tuples\n",
    "    }\n",
    "    \n",
    "    entity_dictionary_list.append(paper_dict)\n",
    "        \n",
    "    print(f\"Successfully appended dictionary for {pmcid_list[i]} to entity_dictionary_list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy of methods for detecting an irregular pulse and suspected atrial fibrillation: A systematic review and meta-analysis',\n",
       " 'Atrial fibrillation and the risk for myocardial infarction, all-cause mortality and heart failure: A systematic review and meta-analysis',\n",
       " 'European Primary Care Cardiovascular Society (EPCCS) consensus guidance on stroke prevention in atrial fibrillation (SPAF) in primary care',\n",
       " 'The effects of rhythm control strategies versus rate control strategies for atrial fibrillation and atrial flutter: A systematic review with meta-analysis and Trial Sequential Analysis',\n",
       " 'Atrial fibrillation and hyperthyroidism: A literature review',\n",
       " 'Patient-Reported Outcomes for Quality of Life Assessment in Atrial Fibrillation: A Systematic Review of Measurement Properties',\n",
       " 'Relation of Body Mass Index With Adverse Outcomes Among Patients With Atrial Fibrillation: A Meta‐Analysis and Systematic Review',\n",
       " 'Measuring the effect of nurse practitioner (NP)-led care on health-related quality of life in adult patients with atrial fibrillation: study protocol for a randomized controlled trial',\n",
       " 'The effects of rhythm control strategies versus rate control strategies for atrial fibrillation and atrial flutter: a protocol for a systematic review with meta-analysis and Trial Sequential Analysis',\n",
       " 'Are Cardiovascular Risk Factors also Associated with the Incidence of Atrial Fibrillation?',\n",
       " 'Body mass index, abdominal fatness, fat mass and the risk of atrial fibrillation: a systematic review and dose–response meta-analysis of prospective studies',\n",
       " 'Vitamin C for preventing atrial fibrillation in high risk patients: a systematic review and meta-analysis',\n",
       " 'The complexity of atrial fibrillation newly diagnosed after ischemic stroke and transient ischemic attack: advances and uncertainties',\n",
       " 'Digoxin versus placebo, no intervention, or other medical interventions for atrial fibrillation and atrial flutter: a protocol for a systematic review with meta-analysis and Trial Sequential Analysis',\n",
       " 'The protective role of small heat shock proteins in cardiac diseases: key role in atrial fibrillation',\n",
       " 'Impact of Contact Force Technology on Atrial Fibrillation Ablation: A Meta-Analysis',\n",
       " 'Effect of RAAS blockers on adverse clinical outcomes in high CVD risk subjects with atrial fibrillation',\n",
       " 'The safety and efficacy of hybrid ablation for the treatment of atrial fibrillation: A meta-analysis',\n",
       " 'Effects of Non–Vitamin K Antagonist Oral Anticoagulants Versus Warfarin in Patients With Atrial Fibrillation and Valvular Heart Disease: A Systematic Review and Meta‐Analysis',\n",
       " 'Decision-Making in Clinical Practice: Oral Anticoagulant Therapy in Patients with Non-valvular Atrial Fibrillation and a Single Additional Stroke Risk Factor',\n",
       " 'Almanac 2015: atrial fibrillation research in Heart',\n",
       " 'Oral anticoagulants for prevention of stroke in atrial fibrillation: systematic review, network meta-analysis, and cost effectiveness analysis',\n",
       " 'Oxidative stress and inflammation as central mediators of atrial fibrillation in obesity and diabetes',\n",
       " 'Novel stroke risk reduction in atrial fibrillation: left atrial appendage occlusion with a focus on the Watchman closure device',\n",
       " 'Pharmacological and Non-pharmacological Treatments for Stroke Prevention in Patients with Atrial Fibrillation',\n",
       " 'How to choose appropriate direct oral anticoagulant for patient with nonvalvular atrial fibrillation',\n",
       " 'Atrial fibrillation as risk factor for cardiovascular disease and death in women compared with men: systematic review and meta-analysis of cohort studies',\n",
       " 'Managing atrial fibrillation in the very elderly patient: challenges and solutions',\n",
       " 'A review of rate control in atrial fibrillation, and the rationale and protocol for the RATE-AF trial',\n",
       " 'Aspirin Compared to Low Intensity Anticoagulation in Patients with Non-Valvular Atrial Fibrillation. A Systematic Review and Meta-Analysis',\n",
       " 'Atrial fibrillation and silent stroke: links, risks, and challenges',\n",
       " 'Aeroembolism in left atrium during catheter ablation of atrial fibrillation in a patient with dextrocardia: a case report and review of the literature',\n",
       " 'Non-Vitamin K Oral Anticoagulants for Stroke Prevention in Special Populations with Atrial Fibrillation',\n",
       " 'Thiazolidinedione use and atrial fibrillation in diabetic patients: a meta-analysis',\n",
       " 'Biomarkers and the prediction of atrial fibrillation: state of the art',\n",
       " 'Patients’ and physicians’ perceptions and attitudes about oral anticoagulation and atrial fibrillation: a qualitative systematic review']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[entity_dictionary['title'] for entity_dictionary in entity_dictionary_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll go ahead and pickle this entity dictionary list, which we can load in our other notebooks, and we also don't have to keep track of how many paper dictionaries we got, since when we load the list in the other notebook, we can just iterate over the list or ask the list how long it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"documents/{noun_for_filename}_review_paper_dictionary_list.pkl\", \"wb\") as picklefile:\n",
    "    pickle.dump(entity_dictionary_list, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atrial_fibrillation'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_for_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
