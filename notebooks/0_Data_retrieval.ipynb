{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import pprint\n",
    "\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../API_ignore.txt\", \"r\") as f:\n",
    "    lines = f.read()\n",
    "\n",
    "entrez_api_key = lines.split(\":\")[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "First, we use esearch to send a query for all reviews & systematic reviews that have free full text for a specific topic; we want to get the PMIDs of these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "esearch_base_query = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?\"\n",
    "    \n",
    "review_pmids_query_dict = {\n",
    "    \"db\": \"pubmed\",\n",
    "    \"sort\": \"relevance\",\n",
    "    \"retmax\": '10',\n",
    "    \"term\": \"{}+AND+((Review[ptyp]+OR+systematic[sb])+AND+free+full+text[sb])\",\n",
    "    \"api_key\": entrez_api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_terms = [k+\"=\"+v for k, v in review_pmids_query_dict.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_review_pmids_query = esearch_base_query + \"&\".join(joined_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&sort=relevance&retmax=10&term={}+AND+((Review[ptyp]+OR+systematic[sb])+AND+free+full+text[sb])&api_key=b0b12c603fda132e7f526bd128008cf75a08'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_review_pmids_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=asthma&Review[ptyp]\"\n",
    "# https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&sort=relevance&retmax=100&term=atrial+fibrillation+AND+((Review[ptyp]+OR+systematic[sb])+AND+free+full+text[sb])\n",
    "\n",
    "#get_review_pmids_query = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={}+AND+((Review[ptyp]+OR+systematic[sb])+AND+free+full+text[sb])&api_key={}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_search = \"atrial+fibrillation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_search = \"lewy+body+dementia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we send this query to pubmed using eutils, we get back an xml object which we can store in a tree.\n",
    "r = requests.get(get_review_pmids_query.format(first_search, entrez_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we send this query to pubmed using eutils, we get back an xml object which we can store in a tree.\n",
    "r = requests.get(get_review_pmids_query.format(second_search, entrez_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.ElementTree(ET.fromstring(r.content))\n",
    "root = tree.getroot()\n",
    "\n",
    "pmids = root.findall('.//Id')\n",
    "\n",
    "pmid_list = [pmid.text for pmid in pmids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Now that we have the PMIDs for the 20 review papers returned by esearch, we have to convert the PMIDs into PMCIDs. In order to convert the PMIDs to PMCIDs, we have to use the ID converter provided by the NCBI, as outlined here: https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert each PMID into a PMCID. The JSON that is returned from this request always has a key 'records'.\n",
    "# Check the dictionary inside of 'records'; if there is a key called 'errmsg', then you know that\n",
    "# the convert request failed. Otherwise, check to see if the dictionary inside of records has a key called \n",
    "# 'pmcid'. If it does, grab the value of the key 'pmcid' and store it. We'll use that PMCID to query PMC to\n",
    "# fetch the xml of the full paper.\n",
    "\n",
    "convert_PMID_query = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=review_assistant&email=jl56923@gmail.com&ids={}&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid_list = []\n",
    "\n",
    "for pmid in pmid_list:\n",
    "    r = requests.get(convert_PMID_query.format(pmid))\n",
    "    result = r.json()\n",
    "    records_dict = result['records'][0]\n",
    "    # If there is an error message in the records dictionary that gets returned with the result, then this\n",
    "    # paper does not have a PMCID and we are not going to be able to get the full text of this paper.\n",
    "    if 'errmsg' in records_dict:\n",
    "        pass\n",
    "    else:\n",
    "        if 'pmcid' in records_dict:\n",
    "            pmcid_list.append(records_dict['pmcid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Now that we have the list of pmcids, we can use efetch to get the xml of these papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pmc_xml_query = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id={}&tool=review_assistant&email=jl56923@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PMC6088773',\n",
       " 'PMC4892610',\n",
       " 'PMC5840831',\n",
       " 'PMC5390937',\n",
       " 'PMC4610749',\n",
       " 'PMC4275567',\n",
       " 'PMC5873980',\n",
       " 'PMC5724510',\n",
       " 'PMC5912679']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmcid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so first finding the abstract and then the body nodes, and then going through each of those and joining together the paragraphs seems to work relatively well. We'll go ahead and write the abstract and body texts to files instead. We'll also define a function that can take an XML node, look for all the paragraphs, join them together and return a clean string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_as_clean_string(xml_node):\n",
    "    # first, we'll clear all the children elements of the table-wrap tags,\n",
    "    # which will get rid of all the content that was in the tables.\n",
    "    # But, first check to see if the table even has tables, because if there is no table-wrap tag\n",
    "    # then you don't need to do anything.\n",
    "    if xml_node.find(\".//table-wrap\"):\n",
    "        for table in xml_node.findall(\".//table-wrap\"):\n",
    "            table.clear()\n",
    "    \n",
    "    node_paragraphs = xml_node.findall(\".//p\")\n",
    "\n",
    "    clean_string = \"\"\n",
    "\n",
    "    for paragraph in node_paragraphs:\n",
    "        clean_string += \" \".join(paragraph.itertext())\n",
    "        clean_string += \" \"\n",
    "        \n",
    "    clean_string = clean_string.strip()\n",
    "    \n",
    "    # We'll get rid of anything inside of square brackets, since those tend to be the citations.\n",
    "    clean_string = re.sub(r'\\[.*?]', \"\", clean_string)\n",
    "    clean_string = re.sub(r'(\\s)+', \" \", clean_string)\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text_exclude_after_conclusion(body_node):\n",
    "    sections = body.findall(\".//sec\")\n",
    "    \n",
    "    conclusion_index = len(sections)\n",
    "    \n",
    "    for index, section in enumerate(sections):\n",
    "        section_title = section.find(\".//title\")\n",
    "        if \"conclusion\" in section_title.text.lower():\n",
    "            conclusion_index = index\n",
    "            break\n",
    "    \n",
    "    article_text = \"\"\n",
    "    \n",
    "    for section in sections[:conclusion_index]:\n",
    "        article_text += get_paragraphs_as_clean_string(section)\n",
    "    \n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pickled dictionary for PMC6088773 as lbd_paper1_dict.pkl.\n",
      "Successfully pickled dictionary for PMC4892610 as lbd_paper2_dict.pkl.\n",
      "Successfully pickled dictionary for PMC5840831 as lbd_paper3_dict.pkl.\n",
      "Unable to get text for PMC5390937.\n",
      "Unable to get text for PMC4610749.\n",
      "Successfully pickled dictionary for PMC4275567 as lbd_paper4_dict.pkl.\n",
      "Unable to get text for PMC5873980.\n",
      "Successfully pickled dictionary for PMC5724510 as lbd_paper5_dict.pkl.\n",
      "Unable to get text for PMC5912679.\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in range(len(pmcid_list)):\n",
    "    \n",
    "    r = requests.get(get_pmc_xml_query.format(pmcid_list[i]))\n",
    "\n",
    "    tree = ET.ElementTree(ET.fromstring(r.content))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Check to see if you can even get the XML from PMC; if not, then pass. If you can, then you can go ahead\n",
    "    # and continue making the dictionary and pickling it.\n",
    "    body = root.find(\".//body\")\n",
    "    if body:\n",
    "        article_text = get_article_text_exclude_after_conclusion(body)\n",
    "    else:\n",
    "        print(f\"Unable to get text for {pmcid_list[i]}.\")\n",
    "        continue\n",
    "    \n",
    "    pmcid = pmcid_list[i]\n",
    "    \n",
    "    # We'll find the first article-title in the text, which should be the title of the paper.\n",
    "    title = root.find(\".//article-title\").text\n",
    "    \n",
    "    keyword_elements = root.findall(\".//kwd\")\n",
    "    keywords = [keyword.text.lower() for keyword in keyword_elements]\n",
    "    \n",
    "    abstract = root.find(\".//abstract\")\n",
    "    abstract_text = get_paragraphs_as_clean_string(abstract)\n",
    "    \n",
    "    citations = root.findall(\".//pub-id\")\n",
    "    citation_tuples = [(citation.text, list(citation.attrib.values())[0]) for citation in citations]\n",
    "    \n",
    "    paper_dict = {\n",
    "        \"pmcid\": pmcid,\n",
    "        \"title\": title,\n",
    "        \"abstract_text\": abstract_text,\n",
    "        \"article_text\": article_text,\n",
    "        \"citation_tuples\": citation_tuples\n",
    "    }\n",
    "    \n",
    "    with open(f\"documents/lbd_paper{j+1}_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(paper_dict, f)\n",
    "        \n",
    "    print(f\"Successfully pickled dictionary for {pmcid_list[i]} as lbd_paper{j+1}_dict.pkl.\")\n",
    "    \n",
    "    j += 1\n",
    "        \n",
    "#     with open(f\"documents/af_paper{i+1}_body.txt\", \"w\") as f:\n",
    "#         f.write(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
